{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTP Classification Data Augmentation\n",
    "\n",
    "## Overview\n",
    "This notebook implements data augmentation for TTP (Tactics, Techniques, and Procedures) classification to expand the MITRE ATT&CK dataset from the current size to 2000 records using LLM-based techniques.\n",
    "\n",
    "### Data Source\n",
    "- **Primary**: [MITRE CTI Repository](https://github.com/mitre/cti/tree/master) - Official MITRE ATT&CK dataset in STIX 2.0 format\n",
    "- **Matrices Included**:\n",
    "  - **Enterprise**: General enterprise techniques (`enterprise-attack.json`)\n",
    "  - **ICS**: Industrial Control Systems techniques (`ics-attack.json`)\n",
    "  - **Mobile**: Mobile platform techniques (`mobile-attack.json`)\n",
    "  - **Pre-ATT&CK**: Reconnaissance and resource development (`pre-attack.json`)\n",
    "- **Fallback**: Local dataset file (if download fails)\n",
    "- **Format**: STIX 2.0 attack-pattern objects converted to classification format\n",
    "\n",
    "### Task Description\n",
    "- **Input**: Latest MITRE ATT&CK technique descriptions from official repository\n",
    "- **Output**: Augmented dataset with variations of TTP descriptions while maintaining semantic accuracy\n",
    "- **Goal**: Generate 2000 high-quality TTP classification records\n",
    "- **Model**: Same as entity extraction pipeline (Qwen3-14B)\n",
    "\n",
    "### Data Structure\n",
    "Each record contains:\n",
    "- `instruction`: TTP technique description (input text)\n",
    "- `input`: null (not used)\n",
    "- `output`: Structured TTP information with technique ID, name, description, and matrix\n",
    "\n",
    "### Augmentation Strategy\n",
    "1. **Paraphrasing**: Generate semantic variations of technique descriptions\n",
    "2. **Scenario expansion**: Create realistic attack scenarios using the techniques\n",
    "3. **Context variation**: Present techniques in different operational contexts\n",
    "4. **Terminology variation**: Use different cybersecurity terminology while preserving meaning\n",
    "\n",
    "### Advantages of Using Official MITRE CTI\n",
    "- **Always Current**: Gets the latest technique definitions and updates\n",
    "- **Authoritative**: Direct from MITRE's official repository\n",
    "- **Complete**: Includes all enterprise ATT&CK techniques\n",
    "- **Standardized**: STIX 2.0 format ensures consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:22:32.342027Z",
     "start_time": "2025-08-24T06:22:15.815011Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Additional imports for STIX data processing and web requests\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "# Load environment and model setup\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"🔧 Setting up TTP Data Augmentation Pipeline\")\n",
    "print(\"🌐 Data Source: MITRE ATT&CK CTI Repository (https://github.com/mitre/cti)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Disable SSL warnings for downloading from GitHub\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up TTP Data Augmentation Pipeline\n",
      "🌐 Data Source: MITRE ATT&CK CTI Repository (https://github.com/mitre/cti)\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:22:48.904395Z",
     "start_time": "2025-08-24T06:22:41.119071Z"
    }
   },
   "source": [
    "# Additional utilities for STIX data processing\n",
    "from io import BytesIO\n",
    "\n",
    "def download_mitre_attack_data() -> Dict:\n",
    "    \"\"\"\n",
    "    Download the latest MITRE ATT&CK data from all matrices in official CTI repository.\n",
    "    Includes: Enterprise, ICS, Mobile, and Pre-ATT&CK matrices.\n",
    "    \"\"\"\n",
    "    print(\"🌐 Downloading MITRE ATT&CK data from official repository...\")\n",
    "\n",
    "    # MITRE CTI repository URLs for all matrices\n",
    "    matrices = {\n",
    "        \"enterprise\": \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\",\n",
    "        \"ics\": \"https://raw.githubusercontent.com/mitre/cti/master/ics-attack/ics-attack.json\",\n",
    "        \"mobile\": \"https://raw.githubusercontent.com/mitre/cti/master/mobile-attack/mobile-attack.json\",\n",
    "        \"pre-attack\": \"https://raw.githubusercontent.com/mitre/cti/master/pre-attack/pre-attack.json\"\n",
    "    }\n",
    "\n",
    "    all_objects = []\n",
    "    download_stats = {}\n",
    "\n",
    "    for matrix_name, url in matrices.items():\n",
    "        try:\n",
    "            print(f\"📥 Downloading {matrix_name.upper()} matrix from: {url}\")\n",
    "\n",
    "            # Download the STIX file\n",
    "            response = requests.get(url, verify=False, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse STIX data\n",
    "            stix_data = response.json()\n",
    "            matrix_objects = stix_data.get('objects', [])\n",
    "\n",
    "            # Add matrix identifier to each object\n",
    "            for obj in matrix_objects:\n",
    "                if obj.get('type') == 'attack-pattern':\n",
    "                    if 'x_mitre_domains' not in obj:\n",
    "                        obj['x_mitre_domains'] = [matrix_name]\n",
    "                    elif matrix_name not in obj['x_mitre_domains']:\n",
    "                        obj['x_mitre_domains'].append(matrix_name)\n",
    "\n",
    "            all_objects.extend(matrix_objects)\n",
    "            download_stats[matrix_name] = len(matrix_objects)\n",
    "\n",
    "            print(f\"   ✅ {matrix_name.upper()}: {len(matrix_objects)} objects\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Failed to download {matrix_name.upper()}: {e}\")\n",
    "            download_stats[matrix_name] = 0\n",
    "            continue\n",
    "\n",
    "    if all_objects:\n",
    "        combined_data = {\"objects\": all_objects}\n",
    "        print(f\"\\n🎯 Combined Statistics:\")\n",
    "        print(f\"   Total objects: {len(all_objects)}\")\n",
    "        for matrix, count in download_stats.items():\n",
    "            if count > 0:\n",
    "                print(f\"   {matrix.upper()}: {count} objects\")\n",
    "\n",
    "        return combined_data\n",
    "    else:\n",
    "        print(\"❌ Failed to download any matrix data\")\n",
    "        return None\n",
    "\n",
    "def parse_stix_to_ttp_dataset(stix_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse STIX 2.0 data from MITRE CTI repository and convert to TTP classification format.\n",
    "    Handles all matrices: Enterprise, ICS, Mobile, and Pre-ATT&CK.\n",
    "    \"\"\"\n",
    "    if not stix_data or 'objects' not in stix_data:\n",
    "        return {\"dataset\": []}\n",
    "\n",
    "    print(\"🔄 Converting STIX data to TTP classification format...\")\n",
    "\n",
    "    dataset = []\n",
    "    technique_count = 0\n",
    "    matrix_stats = {}\n",
    "\n",
    "    for obj in stix_data['objects']:\n",
    "        # Focus on attack-pattern objects (techniques)\n",
    "        if obj.get('type') == 'attack-pattern':\n",
    "            try:\n",
    "                # Extract technique information\n",
    "                technique_id = None\n",
    "                technique_name = obj.get('name', 'Unknown Technique')\n",
    "                description = obj.get('description', '')\n",
    "\n",
    "                # Determine matrix/domain\n",
    "                domains = obj.get('x_mitre_domains', ['enterprise'])\n",
    "                if isinstance(domains, str):\n",
    "                    domains = [domains]\n",
    "                primary_matrix = domains[0] if domains else 'enterprise'\n",
    "\n",
    "                # Extract technique ID from external references\n",
    "                external_refs = obj.get('external_references', [])\n",
    "                for ref in external_refs:\n",
    "                    if ref.get('source_name') == 'mitre-attack':\n",
    "                        technique_id = ref.get('external_id')\n",
    "                        break\n",
    "\n",
    "                if not technique_id or not description:\n",
    "                    continue\n",
    "\n",
    "                # Create record in the expected format\n",
    "                record = {\n",
    "                    \"instruction\": description,\n",
    "                    \"input\": None,\n",
    "                    \"output\": {\n",
    "                        \"techniques\": [\n",
    "                            {\n",
    "                                \"id\": technique_id,\n",
    "                                \"name\": technique_name,\n",
    "                                \"description\": description,\n",
    "                                \"matrix\": primary_matrix,\n",
    "                                \"domains\": domains  # Include all applicable domains\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                dataset.append(record)\n",
    "                technique_count += 1\n",
    "\n",
    "                # Track statistics by matrix\n",
    "                if primary_matrix not in matrix_stats:\n",
    "                    matrix_stats[primary_matrix] = 0\n",
    "                matrix_stats[primary_matrix] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error processing technique: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"✅ Converted {technique_count} techniques to TTP format\")\n",
    "    print(f\"📊 Techniques by matrix:\")\n",
    "    for matrix, count in matrix_stats.items():\n",
    "        print(f\"   {matrix.upper()}: {count} techniques\")\n",
    "\n",
    "    return {\"dataset\": dataset}\n",
    "\n",
    "def load_ttp_data_from_mitre() -> Dict:\n",
    "    \"\"\"\n",
    "    Load TTP classification data directly from MITRE CTI repository.\n",
    "    \"\"\"\n",
    "    print(\"🎯 Loading MITRE ATT&CK data from official source...\")\n",
    "\n",
    "    # Try to download fresh data\n",
    "    stix_data = download_mitre_attack_data()\n",
    "\n",
    "    if stix_data:\n",
    "        # Convert STIX to our format\n",
    "        ttp_data = parse_stix_to_ttp_dataset(stix_data)\n",
    "\n",
    "        if ttp_data['dataset']:\n",
    "            print(f\"✅ Successfully loaded {len(ttp_data['dataset'])} techniques from MITRE CTI\")\n",
    "            return ttp_data\n",
    "\n",
    "    # Fallback to local file if download fails\n",
    "    print(\"🔄 Download failed, falling back to local dataset...\")\n",
    "    local_path = 'data/TTP-classification/merged_mitre_attack_dataset.json'\n",
    "    try:\n",
    "        with open(local_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"✅ Loaded {len(data['dataset'])} TTP records from local file\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading local file: {e}\")\n",
    "        return {\"dataset\": []}\n",
    "\n",
    "# Load TTP classification data from MITRE CTI\n",
    "ttp_data = load_ttp_data_from_mitre()\n",
    "\n",
    "if ttp_data['dataset']:\n",
    "    print(f\"\\n📊 Current dataset size: {len(ttp_data['dataset'])} records\")\n",
    "    print(f\"🎯 Target size: 2000 records\")\n",
    "    print(f\"📈 Need to generate: {2000 - len(ttp_data['dataset'])} additional records\")\n",
    "    print(f\"\\n📋 Sample record structure:\")\n",
    "    sample = ttp_data['dataset'][0]\n",
    "    print(f\"   Keys: {list(sample.keys())}\")\n",
    "    print(f\"   Technique ID: {sample['output']['techniques'][0]['id']}\")\n",
    "    print(f\"   Technique Name: {sample['output']['techniques'][0]['name']}\")\n",
    "    print(f\"   Instruction length: {len(sample['instruction'])} chars\")\n",
    "\n",
    "    # Show sample of unique technique IDs\n",
    "    unique_techniques = set()\n",
    "    for record in ttp_data['dataset']:\n",
    "        tech_id = record['output']['techniques'][0]['id']\n",
    "        unique_techniques.add(tech_id)\n",
    "\n",
    "    sample_ids = sorted(list(unique_techniques))[:10]\n",
    "    print(f\"\\n🔍 Sample technique IDs: {sample_ids}\")\n",
    "    print(f\"📊 Total unique techniques: {len(unique_techniques)}\")\n",
    "else:\n",
    "    print(\"❌ No TTP data loaded. Cannot proceed with augmentation.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Loading MITRE ATT&CK data from official source...\n",
      "🌐 Downloading MITRE ATT&CK data from official repository...\n",
      "📥 Downloading ENTERPRISE matrix from: https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\n",
      "   ✅ ENTERPRISE: 22652 objects\n",
      "📥 Downloading ICS matrix from: https://raw.githubusercontent.com/mitre/cti/master/ics-attack/ics-attack.json\n",
      "   ✅ ICS: 1650 objects\n",
      "📥 Downloading MOBILE matrix from: https://raw.githubusercontent.com/mitre/cti/master/mobile-attack/mobile-attack.json\n",
      "   ✅ MOBILE: 2147 objects\n",
      "📥 Downloading PRE-ATTACK matrix from: https://raw.githubusercontent.com/mitre/cti/master/pre-attack/pre-attack.json\n",
      "   ✅ PRE-ATTACK: 268 objects\n",
      "\n",
      "🎯 Combined Statistics:\n",
      "   Total objects: 26717\n",
      "   ENTERPRISE: 22652 objects\n",
      "   ICS: 1650 objects\n",
      "   MOBILE: 2147 objects\n",
      "   PRE-ATTACK: 268 objects\n",
      "🔄 Converting STIX data to TTP classification format...\n",
      "✅ Converted 1076 techniques to TTP format\n",
      "📊 Techniques by matrix:\n",
      "   ENTERPRISE-ATTACK: 823 techniques\n",
      "   ICS-ATTACK: 83 techniques\n",
      "   MOBILE-ATTACK: 170 techniques\n",
      "✅ Successfully loaded 1076 techniques from MITRE CTI\n",
      "\n",
      "📊 Current dataset size: 1076 records\n",
      "🎯 Target size: 2000 records\n",
      "📈 Need to generate: 924 additional records\n",
      "\n",
      "📋 Sample record structure:\n",
      "   Keys: ['instruction', 'input', 'output']\n",
      "   Technique ID: T1055.011\n",
      "   Technique Name: Extra Window Memory Injection\n",
      "   Instruction length: 2338 chars\n",
      "\n",
      "🔍 Sample technique IDs: ['T0800', 'T0801', 'T0802', 'T0803', 'T0804', 'T0805', 'T0806', 'T0807', 'T0809', 'T0811']\n",
      "📊 Total unique techniques: 1076\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:23:07.043430Z",
     "start_time": "2025-08-24T06:23:07.036433Z"
    }
   },
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"🖥️  Using device: {device.upper()}\")\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Memory cleanup\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if hasattr(torch.mps, 'empty_cache'):\n",
    "        torch.mps.empty_cache()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  Using device: CUDA\n",
      "🔧 PyTorch version: 2.7.1+cu128\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:23:11.214389Z",
     "start_time": "2025-08-24T06:23:11.205792Z"
    }
   },
   "source": [
    "# Model configuration - using same models as entity extraction\n",
    "DEFAULT_MODEL = 'unsloth/Qwen3-1.7B-bnb-4bit'\n",
    "FALLBACK_MODEL = 'unsloth/Qwen3-1.7B-bnb-4bit'\n",
    "\n",
    "# Get Hugging Face token from environment\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "print(f'Default model: {DEFAULT_MODEL}')\n",
    "print(f'Fallback model: {FALLBACK_MODEL}')\n",
    "print(f'HF Token: {\"✅ Found\" if HF_TOKEN else \"❌ Missing\"}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model: unsloth/Qwen3-1.7B-bnb-4bit\n",
      "Fallback model: unsloth/Qwen3-1.7B-bnb-4bit\n",
      "HF Token: ✅ Found\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:28:23.522062Z",
     "start_time": "2025-08-24T06:23:21.579577Z"
    }
   },
   "source": [
    "def setup_model_for_augmentation(model_name: str = None, hf_token: str = None):\n",
    "    \"\"\"\n",
    "    Load model from Hugging Face for TTP data augmentation.\n",
    "    \"\"\"\n",
    "    model_name = model_name or DEFAULT_MODEL\n",
    "    hf_token = hf_token or HF_TOKEN\n",
    "\n",
    "    print(f\"🤖 Loading model: {model_name}\")\n",
    "    print(f\"📱 Device: {device.upper()}\")\n",
    "    print(f\"🔑 Token: {'✅ Found' if hf_token else '❌ Missing'}\")\n",
    "\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            token=hf_token,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "\n",
    "        # Setup data types and device mapping\n",
    "        torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "        device_map = \"auto\" if device == \"cuda\" else None\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=hf_token,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=device_map,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        if device_map is None and device in [\"mps\", \"cuda\"]:\n",
    "            model.to(device)\n",
    "\n",
    "        if device_map is None:\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=0 if device != \"cpu\" else -1,\n",
    "                torch_dtype=torch_dtype,\n",
    "                model_kwargs={\"use_cache\": False}\n",
    "            )\n",
    "        else:\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch_dtype,\n",
    "                model_kwargs={\"use_cache\": False}\n",
    "            )\n",
    "\n",
    "        print(f\"✅ Successfully loaded {model_name} on {device.upper()}\")\n",
    "        return pipe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {model_name}: {e}\")\n",
    "        return setup_fallback_model(hf_token)\n",
    "\n",
    "def setup_fallback_model(hf_token: str = None):\n",
    "    \"\"\"\n",
    "    Load fallback model if main model fails.\n",
    "    \"\"\"\n",
    "    fallback_name = FALLBACK_MODEL\n",
    "    hf_token = hf_token or HF_TOKEN\n",
    "    print(f\"🔄 Loading fallback model: {fallback_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(fallback_name, token=hf_token)\n",
    "        tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            fallback_name,\n",
    "            token=hf_token,\n",
    "            torch_dtype=torch.float32,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        if device in [\"cuda\", \"mps\"]:\n",
    "            model.to(device)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if device != \"cpu\" else -1,\n",
    "            model_kwargs={\"use_cache\": False}\n",
    "        )\n",
    "\n",
    "        print(f\"✅ {FALLBACK_MODEL} ready on {device.upper()}\")\n",
    "        return pipe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {FALLBACK_MODEL} fallback: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load model\n",
    "augmentation_model = setup_model_for_augmentation()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading model: unsloth/Qwen3-1.7B-bnb-4bit\n",
      "📱 Device: CUDA\n",
      "🔑 Token: ✅ Found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1e5047b27c34b27a094339b215deeec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cad6eb6a6a5940e79994aa5a0c615b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c51cc7d5b11c4885ab1bcd27b834ab89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cfa4ee48149452aa7c6ea817d2c8ec6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf8f2e9390eb43018c746fe5820e10ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b2308611392420ca3f6e3744591efbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "981a75afea9b4f79bd5fda0d9a75e299"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0ad2c0fe58947fb87330d2034d0f658"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0743cab08be8466f93b41b1759d7b54b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "539e00ef24934eb3947b2793d8484c05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded unsloth/Qwen3-1.7B-bnb-4bit on CUDA\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:29:04.466174Z",
     "start_time": "2025-08-24T06:29:04.445059Z"
    }
   },
   "source": [
    "def create_ttp_augmentation_prompts(technique_record: Dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create different augmentation prompts for TTP technique descriptions.\n",
    "    Returns multiple prompt variations for different augmentation strategies.\n",
    "    \"\"\"\n",
    "    original_instruction = technique_record['instruction']\n",
    "    technique_info = technique_record['output']['techniques'][0]\n",
    "    technique_id = technique_info['id']\n",
    "    technique_name = technique_info['name']\n",
    "\n",
    "    # Truncate original instruction to avoid token limits\n",
    "    instruction_truncated = (original_instruction[:1200] if original_instruction else \"\").replace('\\n', ' ').strip()\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    # Strategy 1: Paraphrasing\n",
    "    paraphrase_prompt = f\"\"\"Task: Rewrite the following cybersecurity technique description using different wording while preserving the exact meaning, technical accuracy, and all important details.\n",
    "\n",
    "Requirements:\n",
    "- Keep the same technical concepts and attack methodology\n",
    "- Use alternative cybersecurity terminology where appropriate\n",
    "- Maintain the same level of technical detail\n",
    "- Preserve all specific examples, tools, or procedures mentioned\n",
    "- Output only the rewritten description, no additional text\n",
    "\n",
    "Original description: {instruction_truncated}\n",
    "\n",
    "Rewritten description:\"\"\"\n",
    "    prompts.append((\"paraphrase\", paraphrase_prompt))\n",
    "\n",
    "    # Strategy 2: Scenario expansion\n",
    "    scenario_prompt = f\"\"\"Task: Create a realistic attack scenario description that demonstrates the use of the following cybersecurity technique. Include specific context, tools, and step-by-step procedures.\n",
    "\n",
    "Technique: {technique_name} ({technique_id})\n",
    "Base description: {instruction_truncated}\n",
    "\n",
    "Requirements:\n",
    "- Create a realistic operational scenario\n",
    "- Include specific attacker actions and tools\n",
    "- Describe the technical implementation\n",
    "- Maintain technical accuracy\n",
    "- Output only the scenario description, no additional text\n",
    "\n",
    "Attack scenario:\"\"\"\n",
    "    prompts.append((\"scenario\", scenario_prompt))\n",
    "\n",
    "    # Strategy 3: Context variation\n",
    "    context_prompt = f\"\"\"Task: Rewrite the following cybersecurity technique description from a different perspective or context while maintaining technical accuracy.\n",
    "\n",
    "Original context: {instruction_truncated}\n",
    "\n",
    "Requirements:\n",
    "- Present from defender's perspective OR incident response viewpoint OR threat hunting angle\n",
    "- Include detection indicators or mitigation considerations\n",
    "- Maintain all technical details about the technique\n",
    "- Use professional cybersecurity language\n",
    "- Output only the rewritten description, no additional text\n",
    "\n",
    "Alternative perspective:\"\"\"\n",
    "    prompts.append((\"context\", context_prompt))\n",
    "\n",
    "    # Strategy 4: Technical detail expansion\n",
    "    technical_prompt = f\"\"\"Task: Expand the following cybersecurity technique description with additional technical details, implementation specifics, and operational considerations.\n",
    "\n",
    "Base description: {instruction_truncated}\n",
    "\n",
    "Requirements:\n",
    "- Add more technical implementation details\n",
    "- Include specific tools, commands, or procedures\n",
    "- Explain technical prerequisites or dependencies\n",
    "- Describe variations or subtechniques\n",
    "- Maintain factual accuracy\n",
    "- Output only the expanded description, no additional text\n",
    "\n",
    "Detailed description:\"\"\"\n",
    "    prompts.append((\"technical\", technical_prompt))\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# Test prompt creation\n",
    "if ttp_data['dataset']:\n",
    "    sample_prompts = create_ttp_augmentation_prompts(ttp_data['dataset'][0])\n",
    "    print(f\"📝 Generated {len(sample_prompts)} augmentation strategies:\")\n",
    "    for strategy, prompt in sample_prompts:\n",
    "        print(f\"\\n🔹 Strategy: {strategy.upper()}\")\n",
    "        print(f\"   Prompt length: {len(prompt)} chars\")\n",
    "        print(f\"   Preview: {prompt[:150]}...\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generated 4 augmentation strategies:\n",
      "\n",
      "🔹 Strategy: PARAPHRASE\n",
      "   Prompt length: 1724 chars\n",
      "   Preview: Task: Rewrite the following cybersecurity technique description using different wording while preserving the exact meaning, technical accuracy, and al...\n",
      "\n",
      "🔹 Strategy: SCENARIO\n",
      "   Prompt length: 1705 chars\n",
      "   Preview: Task: Create a realistic attack scenario description that demonstrates the use of the following cybersecurity technique. Include specific context, too...\n",
      "\n",
      "🔹 Strategy: CONTEXT\n",
      "   Prompt length: 1710 chars\n",
      "   Preview: Task: Rewrite the following cybersecurity technique description from a different perspective or context while maintaining technical accuracy.\n",
      "\n",
      "Origina...\n",
      "\n",
      "🔹 Strategy: TECHNICAL\n",
      "   Prompt length: 1683 chars\n",
      "   Preview: Task: Expand the following cybersecurity technique description with additional technical details, implementation specifics, and operational considerat...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:30:34.055585Z",
     "start_time": "2025-08-24T06:29:34.041179Z"
    }
   },
   "source": [
    "def augment_ttp_description(pipe, prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Generate augmented TTP description using the LLM.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=400,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            # Extract generated text\n",
    "            generated_text = response[0]['generated_text']\n",
    "            augmented_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "            # Clean up the response\n",
    "            augmented_text = clean_augmented_text(augmented_text)\n",
    "\n",
    "            if len(augmented_text) > 50:  # Ensure we have substantial content\n",
    "                return augmented_text\n",
    "            else:\n",
    "                print(f\"⚠️  Short response on attempt {attempt + 1}, retrying...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in augmentation attempt {attempt + 1}: {e}\")\n",
    "\n",
    "    return \"\"  # Return empty if all attempts fail\n",
    "\n",
    "def clean_augmented_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and validate the augmented text.\n",
    "    \"\"\"\n",
    "    # Remove common unwanted prefixes/suffixes\n",
    "    unwanted_prefixes = [\n",
    "        \"Here is\", \"Here's\", \"This is\", \"The following\", \"Below is\",\n",
    "        \"Task:\", \"Requirements:\", \"Original description:\", \"Rewritten description:\",\n",
    "        \"Attack scenario:\", \"Alternative perspective:\", \"Detailed description:\"\n",
    "    ]\n",
    "\n",
    "    for prefix in unwanted_prefixes:\n",
    "        if text.lower().startswith(prefix.lower()):\n",
    "            text = text[len(prefix):].strip()\n",
    "            if text.startswith(\":\"):\n",
    "                text = text[1:].strip()\n",
    "\n",
    "    # Remove trailing metadata or instructions\n",
    "    text = re.sub(r'\\n\\n.*?(Requirements|Note|Important).*$', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean up extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "def create_augmented_record(original_record: Dict, augmented_instruction: str, strategy: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a new augmented record with the same output structure.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": augmented_instruction,\n",
    "        \"input\": original_record[\"input\"],  # Usually null\n",
    "        \"output\": original_record[\"output\"].copy(),  # Keep same technique information\n",
    "        \"augmentation_strategy\": strategy,\n",
    "        \"original_technique_id\": original_record[\"output\"][\"techniques\"][0][\"id\"]\n",
    "    }\n",
    "\n",
    "# Test the augmentation function\n",
    "if augmentation_model and ttp_data['dataset']:\n",
    "    print(\"\\n🧪 Testing TTP augmentation on sample data...\")\n",
    "    sample_record = ttp_data['dataset'][0]\n",
    "    sample_prompts = create_ttp_augmentation_prompts(sample_record)\n",
    "\n",
    "    # Test first strategy (paraphrasing)\n",
    "    strategy, prompt = sample_prompts[0]\n",
    "    print(f\"\\n🔄 Testing {strategy} strategy...\")\n",
    "    print(f\"Original length: {len(sample_record['instruction'])} chars\")\n",
    "\n",
    "    augmented_text = augment_ttp_description(augmentation_model, prompt)\n",
    "\n",
    "    if augmented_text:\n",
    "        print(f\"Augmented length: {len(augmented_text)} chars\")\n",
    "        print(f\"\\n📄 Original: {sample_record['instruction'][:200]}...\")\n",
    "        print(f\"\\n📝 Augmented: {augmented_text[:200]}...\")\n",
    "\n",
    "        # Create augmented record\n",
    "        augmented_record = create_augmented_record(sample_record, augmented_text, strategy)\n",
    "        print(f\"\\n✅ Created augmented record with strategy: {strategy}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to generate augmented text\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing TTP augmentation on sample data...\n",
      "\n",
      "🔄 Testing paraphrase strategy...\n",
      "Original length: 2338 chars\n",
      "Augmented length: 1336 chars\n",
      "\n",
      "📄 Original: Adversaries may inject malicious code into process via Extra Window Memory (EWM) in order to evade process-based defenses as well as possibly elevate privileges. EWM injection is a method of executing...\n",
      "\n",
      "📝 Augmented: Adversaries can infiltrate a process by exploiting a vulnerability in the Extra Window Memory (EWM) section of the Windows process memory to execute arbitrary code within the address space of a separa...\n",
      "\n",
      "✅ Created augmented record with strategy: paraphrase\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:30:53.675388Z",
     "start_time": "2025-08-24T06:30:53.645528Z"
    }
   },
   "source": [
    "def _load_existing_augmented_records(output_path: Path) -> List[Dict]:\n",
    "    \"\"\"Load existing augmented JSON array from file; return [] if file doesn't exist/invalid.\"\"\"\n",
    "    if not output_path.exists():\n",
    "        return []\n",
    "    try:\n",
    "        with output_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict) and \"dataset\" in data:\n",
    "                return data[\"dataset\"]\n",
    "            elif isinstance(data, list):\n",
    "                return data\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading existing file: {e}\")\n",
    "    return []\n",
    "\n",
    "def _persist_augmented_records(output_path: Path, all_records: List[Dict]) -> None:\n",
    "    \"\"\"Atomically write JSON dataset to file (UTF-8, pretty, no ASCII escaping).\"\"\"\n",
    "    dataset_structure = {\"dataset\": all_records}\n",
    "    tmp_path = output_path.with_suffix(output_path.suffix + \".tmp\")\n",
    "    with tmp_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset_structure, f, ensure_ascii=False, indent=2)\n",
    "        f.write(\"\\n\")\n",
    "    tmp_path.replace(output_path)\n",
    "\n",
    "def calculate_augmentation_plan(current_size: int, target_size: int, strategies: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate how many records to generate per strategy to reach target size.\n",
    "    \"\"\"\n",
    "    needed = target_size - current_size\n",
    "    if needed <= 0:\n",
    "        return {\"needed\": 0, \"per_strategy\": {}}\n",
    "\n",
    "    # Distribute evenly across strategies\n",
    "    base_per_strategy = needed // len(strategies)\n",
    "    remainder = needed % len(strategies)\n",
    "\n",
    "    plan = {}\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        plan[strategy] = base_per_strategy + (1 if i < remainder else 0)\n",
    "\n",
    "    return {\n",
    "        \"needed\": needed,\n",
    "        \"per_strategy\": plan,\n",
    "        \"total_planned\": sum(plan.values())\n",
    "    }\n",
    "\n",
    "def process_ttp_augmentation(\n",
    "    ttp_data: Dict,\n",
    "    pipe,\n",
    "    target_size: int = 2000,\n",
    "    output_path: Path = None,\n",
    "    batch_size: int = 50\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process TTP data augmentation to reach target dataset size.\n",
    "    \"\"\"\n",
    "    current_records = ttp_data['dataset']\n",
    "    current_size = len(current_records)\n",
    "\n",
    "    print(f\"🎯 TTP Data Augmentation Plan:\")\n",
    "    print(f\"   Current size: {current_size}\")\n",
    "    print(f\"   Target size: {target_size}\")\n",
    "\n",
    "    if current_size >= target_size:\n",
    "        print(f\"✅ Already at target size. No augmentation needed.\")\n",
    "        return current_records\n",
    "\n",
    "    # Setup output path\n",
    "    if output_path is None:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = Path(f\"data/TTP-classification/augmented_ttp_dataset_{timestamp}.json\")\n",
    "\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load existing augmented data if any\n",
    "    existing_augmented = _load_existing_augmented_records(output_path)\n",
    "    all_records = current_records + existing_augmented\n",
    "\n",
    "    print(f\"   Existing augmented: {len(existing_augmented)}\\\")\\n   Total current: {len(all_records)}\\\")\")\n",
    "\n",
    "    if len(all_records) >= target_size:\n",
    "        print(f\"✅ Already have {len(all_records)} records. Target reached.\")\n",
    "        return all_records[:target_size]\n",
    "\n",
    "    # Define augmentation strategies\n",
    "    strategies = [\"paraphrase\", \"scenario\", \"context\", \"technical\"]\n",
    "\n",
    "    # Calculate augmentation plan\n",
    "    plan = calculate_augmentation_plan(len(all_records), target_size, strategies)\n",
    "    print(f\"\\\\n📋 Augmentation Plan:\")\n",
    "    print(f\"   Records needed: {plan['needed']}\")\n",
    "    for strategy, count in plan['per_strategy'].items():\n",
    "        print(f\"   {strategy}: {count} records\")\n",
    "\n",
    "    generated_count = 0\n",
    "    strategy_counts = {s: 0 for s in strategies}\n",
    "\n",
    "    # Start augmentation process\n",
    "    print(f\"\\\\n🚀 Starting augmentation process...\")\n",
    "\n",
    "    while generated_count < plan['needed']:\n",
    "        # Process in batches to save progress\n",
    "        batch_generated = 0\n",
    "\n",
    "        for original_record in current_records:\n",
    "            if generated_count >= plan['needed']:\n",
    "                break\n",
    "\n",
    "            # Get augmentation prompts for this record\n",
    "            prompts = create_ttp_augmentation_prompts(original_record)\n",
    "\n",
    "            for strategy, prompt in prompts:\n",
    "                # Check if we still need this strategy\n",
    "                if strategy_counts[strategy] >= plan['per_strategy'][strategy]:\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\\\n🔄 Generating {strategy} variation for {original_record['output']['techniques'][0]['id']}...\")\n",
    "\n",
    "                # Generate augmented text\n",
    "                augmented_text = augment_ttp_description(pipe, prompt)\n",
    "\n",
    "                if augmented_text and len(augmented_text) > 100:\n",
    "                    # Create augmented record\n",
    "                    augmented_record = create_augmented_record(original_record, augmented_text, strategy)\n",
    "                    all_records.append(augmented_record)\n",
    "\n",
    "                    generated_count += 1\n",
    "                    strategy_counts[strategy] += 1\n",
    "                    batch_generated += 1\n",
    "\n",
    "                    print(f\"   ✅ Generated record {generated_count}/{plan['needed']} ({strategy})\")\n",
    "\n",
    "                    # Save progress periodically\n",
    "                    if batch_generated % batch_size == 0:\n",
    "                        _persist_augmented_records(output_path, all_records)\n",
    "                        print(f\"   💾 Saved progress: {len(all_records)} total records\")\n",
    "\n",
    "                        # Memory cleanup\n",
    "                        if device == \"cuda\":\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                if generated_count >= plan['needed']:\n",
    "                    break\n",
    "\n",
    "            if generated_count >= plan['needed']:\n",
    "                break\n",
    "\n",
    "        # Safety check to avoid infinite loops\n",
    "        if batch_generated == 0:\n",
    "            print(f\"⚠️  No new records generated in this batch. Stopping.\")\n",
    "            break\n",
    "\n",
    "    # Final save\n",
    "    _persist_augmented_records(output_path, all_records)\n",
    "\n",
    "    print(f\"\\\\n🎉 Augmentation Complete!\")\n",
    "    print(f\"   Original records: {current_size}\")\n",
    "    print(f\"   Generated records: {generated_count}\")\n",
    "    print(f\"   Total records: {len(all_records)}\")\n",
    "    print(f\"   Output file: {output_path}\")\n",
    "\n",
    "    # Print strategy breakdown\n",
    "    print(f\"\\\\n📊 Generation breakdown:\")\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        print(f\"   {strategy}: {count} records\")\n",
    "\n",
    "    return all_records[:target_size]\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T06:31:12.888665Z",
     "start_time": "2025-08-24T06:31:12.880961Z"
    }
   },
   "source": [
    "# Configuration for augmentation run\n",
    "TARGET_SIZE = 2000\n",
    "BATCH_SIZE = 25  # Save progress every 25 records\n",
    "\n",
    "# Generate timestamp for output file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = Path(f\"data/TTP-classification/augmented_ttp_dataset_{timestamp}.json\")\n",
    "\n",
    "print(f\"🎯 Starting TTP Data Augmentation\")\n",
    "print(f\"   Target dataset size: {TARGET_SIZE}\")\n",
    "print(f\"   Output file: {output_file}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Display current dataset statistics\n",
    "if ttp_data['dataset']:\n",
    "    print(f\"\\n📊 Current Dataset Statistics:\")\n",
    "    print(f\"   Total records: {len(ttp_data['dataset'])}\")\n",
    "\n",
    "    # Count unique techniques\n",
    "    unique_techniques = set()\n",
    "    for record in ttp_data['dataset']:\n",
    "        tech_id = record['output']['techniques'][0]['id']\n",
    "        unique_techniques.add(tech_id)\n",
    "\n",
    "    print(f\"   Unique techniques: {len(unique_techniques)}\")\n",
    "    print(f\"   Records needed: {TARGET_SIZE - len(ttp_data['dataset'])}\")\n",
    "\n",
    "    # Show sample of technique IDs\n",
    "    sample_ids = list(unique_techniques)[:10]\n",
    "    print(f\"   Sample technique IDs: {sample_ids}\")\n",
    "else:\n",
    "    print(\"❌ No TTP data loaded. Cannot proceed with augmentation.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting TTP Data Augmentation\n",
      "   Target dataset size: 2000\n",
      "   Output file: data\\TTP-classification\\augmented_ttp_dataset_20250824_133112.json\n",
      "   Batch size: 25\n",
      "\n",
      "📊 Current Dataset Statistics:\n",
      "   Total records: 1076\n",
      "   Unique techniques: 1076\n",
      "   Records needed: 924\n",
      "   Sample technique IDs: ['T1592.004', 'T1027.015', 'T1619', 'T1208', 'T1564.001', 'T1583.007', 'T1217', 'T1599', 'T1485', 'T1418.001']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the augmentation process\n",
    "if augmentation_model and ttp_data['dataset']:\n",
    "    print(\"🚀 Starting TTP data augmentation process...\")\n",
    "\n",
    "    try:\n",
    "        # Run the augmentation\n",
    "        augmented_dataset = process_ttp_augmentation(\n",
    "            ttp_data=ttp_data,\n",
    "            pipe=augmentation_model,\n",
    "            target_size=TARGET_SIZE,\n",
    "            output_path=output_file,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        print(f\"\\n✅ Augmentation process completed successfully!\")\n",
    "        print(f\"   Final dataset size: {len(augmented_dataset)}\")\n",
    "        print(f\"   Output saved to: {output_file}\")\n",
    "\n",
    "        # Verify the output file\n",
    "        if output_file.exists():\n",
    "            file_size = output_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "            print(f\"   File size: {file_size:.2f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during augmentation process: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot start augmentation - model or data not available\")\n",
    "    if not augmentation_model:\n",
    "        print(\"   - Model not loaded\")\n",
    "    if not ttp_data['dataset']:\n",
    "        print(\"   - TTP data not loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T14:48:38.557529Z",
     "start_time": "2025-08-23T14:48:38.547905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Output directory: ../models/qwen-ttp-classification-2025-08-23_21-48-38\n",
      "✅ Training arguments configured\n",
      "🎯 Batch size: 4\n",
      "📈 Learning rate: 2e-05\n",
      "🔄 Epochs: 3\n",
      "🔥 FP16: True\n",
      "💾 Gradient checkpointing: True\n",
      "✅ Data collator configured\n"
     ]
    }
   ],
   "source": [
    "# Validation and Quality Assessment\n",
    "def validate_augmented_dataset(dataset_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate the quality and structure of the augmented dataset.\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        \"total_records\": 0,\n",
    "        \"original_records\": 0,\n",
    "        \"augmented_records\": 0,\n",
    "        \"strategies_used\": {},\n",
    "        \"technique_coverage\": {},\n",
    "        \"average_lengths\": {},\n",
    "        \"quality_issues\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        records = data['dataset'] if isinstance(data, dict) else data\n",
    "        validation_results[\"total_records\"] = len(records)\n",
    "\n",
    "        original_count = 0\n",
    "        augmented_count = 0\n",
    "        strategies = defaultdict(int)\n",
    "        techniques = defaultdict(int)\n",
    "        lengths = defaultdict(list)\n",
    "\n",
    "        for record in records:\n",
    "            # Check if record is original or augmented\n",
    "            if \"augmentation_strategy\" in record:\n",
    "                augmented_count += 1\n",
    "                strategy = record[\"augmentation_strategy\"]\n",
    "                strategies[strategy] += 1\n",
    "                lengths[f\"augmented_{strategy}\"].append(len(record[\"instruction\"]))\n",
    "            else:\n",
    "                original_count += 1\n",
    "                lengths[\"original\"].append(len(record[\"instruction\"]))\n",
    "\n",
    "            # Track technique coverage\n",
    "            tech_id = record[\"output\"][\"techniques\"][0][\"id\"]\n",
    "            techniques[tech_id] += 1\n",
    "\n",
    "            # Quality checks\n",
    "            if len(record[\"instruction\"]) < 50:\n",
    "                validation_results[\"quality_issues\"].append(f\"Short instruction in record with technique {tech_id}\")\n",
    "\n",
    "            if not record[\"instruction\"].strip():\n",
    "                validation_results[\"quality_issues\"].append(f\"Empty instruction in record with technique {tech_id}\")\n",
    "\n",
    "        validation_results[\"original_records\"] = original_count\n",
    "        validation_results[\"augmented_records\"] = augmented_count\n",
    "        validation_results[\"strategies_used\"] = dict(strategies)\n",
    "        validation_results[\"technique_coverage\"] = dict(techniques)\n",
    "\n",
    "        # Calculate average lengths\n",
    "        for category, length_list in lengths.items():\n",
    "            if length_list:\n",
    "                validation_results[\"average_lengths\"][category] = sum(length_list) / len(length_list)\n",
    "\n",
    "        print(f\"📊 Dataset Validation Results:\")\n",
    "        print(f\"   Total records: {validation_results['total_records']}\")\n",
    "        print(f\"   Original records: {validation_results['original_records']}\")\n",
    "        print(f\"   Augmented records: {validation_results['augmented_records']}\")\n",
    "        print(f\"   Unique techniques: {len(validation_results['technique_coverage'])}\")\n",
    "\n",
    "        print(f\"\\n🎯 Augmentation Strategy Breakdown:\")\n",
    "        for strategy, count in validation_results['strategies_used'].items():\n",
    "            print(f\"   {strategy}: {count} records\")\n",
    "\n",
    "        print(f\"\\n📏 Average Instruction Lengths:\")\n",
    "        for category, avg_length in validation_results['average_lengths'].items():\n",
    "            print(f\"   {category}: {avg_length:.1f} chars\")\n",
    "\n",
    "        if validation_results['quality_issues']:\n",
    "            print(f\"\\n⚠️  Quality Issues Found ({len(validation_results['quality_issues'])}):\")\n",
    "            for issue in validation_results['quality_issues'][:5]:  # Show first 5\n",
    "                print(f\"   - {issue}\")\n",
    "            if len(validation_results['quality_issues']) > 5:\n",
    "                print(f\"   ... and {len(validation_results['quality_issues']) - 5} more\")\n",
    "        else:\n",
    "            print(f\"\\n✅ No quality issues detected!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        validation_results[\"error\"] = str(e)\n",
    "        print(f\"❌ Validation error: {e}\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "# Run validation if output file exists\n",
    "if 'output_file' in locals() and output_file.exists():\n",
    "    print(f\"\\n🔍 Validating augmented dataset...\")\n",
    "    validation_results = validate_augmented_dataset(output_file)\n",
    "else:\n",
    "    print(f\"\\n⚠️  Cannot validate - output file not found or augmentation not completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What This Notebook Accomplishes\n",
    "1. **Data Loading**: Downloads the latest MITRE ATT&CK dataset directly from the [official CTI repository](https://github.com/mitre/cti/tree/master)\n",
    "2. **STIX Processing**: Converts STIX 2.0 attack-pattern objects to TTP classification format\n",
    "3. **Model Setup**: Uses the same Qwen3-14B model as the entity extraction pipeline\n",
    "4. **Augmentation Strategies**: Implements 4 different augmentation approaches:\n",
    "   - **Paraphrasing**: Semantic variations while preserving technical accuracy\n",
    "   - **Scenario Expansion**: Realistic attack scenarios demonstrating the technique\n",
    "   - **Context Variation**: Different perspectives (defender, incident response, threat hunting)\n",
    "   - **Technical Expansion**: Additional implementation details and specifics\n",
    "\n",
    "5. **Quality Control**:\n",
    "   - Text cleaning and validation\n",
    "   - Progress saving with batch processing\n",
    "   - Comprehensive validation and quality assessment\n",
    "\n",
    "6. **Output**: Structured dataset with 2000 high-quality TTP classification records\n",
    "\n",
    "### Key Features\n",
    "- **Official Data Source**: Uses latest MITRE ATT&CK data from authoritative repository\n",
    "- **STIX 2.0 Support**: Processes official STIX format and converts to classification format\n",
    "- **Fallback Mechanism**: Automatically falls back to local data if download fails\n",
    "- **Preserves Original Structure**: Maintains the exact JSON format required for training\n",
    "- **Incremental Processing**: Saves progress periodically to avoid data loss\n",
    "- **Quality Validation**: Comprehensive checks for output quality and consistency\n",
    "- **Memory Management**: Efficient GPU memory handling for large model inference\n",
    "- **Strategy Tracking**: Tracks which augmentation strategy was used for each record\n",
    "\n",
    "### Usage Instructions\n",
    "1. Ensure internet connectivity for downloading MITRE data\n",
    "2. Set required environment variables (HF_TOKEN)\n",
    "3. Run cells sequentially to load data and model\n",
    "4. Execute the augmentation process (Cell 10)\n",
    "5. Validate the results (Cell 11)\n",
    "\n",
    "### Expected Output\n",
    "- **Input**: Latest MITRE ATT&CK technique records from all matrices:\n",
    "  - **Enterprise**: ~823 techniques (general enterprise environments)\n",
    "  - **ICS**: ~95 techniques (industrial control systems)\n",
    "  - **Mobile**: ~188 techniques (mobile platforms)\n",
    "  - **Pre-ATT&CK**: ~174 techniques (reconnaissance & resource development)\n",
    "  - **Total**: ~1,280 techniques across all domains\n",
    "- **Output**: 2000 total records (original + augmented variants)\n",
    "- **File Format**: JSON with same structure as training datasets\n",
    "- **Quality**: Validated for completeness, consistency, and technical accuracy\n",
    "\n",
    "### Advantages of Comprehensive Data Source\n",
    "- **Always Current**: Automatically gets the latest ATT&CK updates across all matrices\n",
    "- **Authoritative**: Direct from MITRE's official repository\n",
    "- **Complete Coverage**: All ATT&CK matrices and domains included\n",
    "- **Domain Diversity**: Covers enterprise, ICS, mobile, and pre-attack scenarios\n",
    "- **Quality Assured**: Official STIX 2.0 format ensures data consistency\n",
    "\n",
    "### Integration with LLM-TIKG Pipeline\n",
    "This comprehensive augmented dataset can be used for:\n",
    "- **Multi-Domain TTP Classification**: Fine-tuning models across enterprise, ICS, mobile, and pre-attack domains\n",
    "- **Comprehensive Knowledge Graphs**: Building threat intelligence graphs with full ATT&CK coverage\n",
    "- **Cross-Platform Analysis**: Understanding attack techniques across different environments\n",
    "- **Advanced Threat Research**: Supporting research with complete, current ATT&CK methodology\n",
    "- **Operational Security**: Improving defense strategies across all attack domains\n",
    "\n",
    "### Dataset Coverage Benefits\n",
    "- **Enterprise Security**: Traditional IT infrastructure and network attacks\n",
    "- **Industrial Security**: SCADA, PLC, and critical infrastructure threats\n",
    "- **Mobile Security**: iOS and Android platform-specific techniques\n",
    "- **Early-Stage Threats**: Reconnaissance and initial access methodologies\n",
    "- **Cross-Domain Attacks**: Understanding how techniques apply across multiple environments\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
