{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Finetune — Qwen3 for NER (Entity Extraction) ➜ then TTP Classification\n",
        "\n",
        "**Models**:\n",
        "- Base A (1.7B): `unsloth/Qwen3-1.7B-bnb-4bit`\n",
        "- Base B (4B): `unsloth/Qwen3-4B-bnb-4bit` (baseline zero-shot only)\n",
        "\n",
        "**Pipeline**:\n",
        "1. Datasets for **NER** @entity-extraction/ and **TTP** @TTP-classification/\n",
        "2. Evaluate **zero-shot**: 1.7B and 4B\n",
        "3. **Finetune 1.7B** on NER (QLoRA), save adapter\n",
        "4. **Continue finetune** (same adapter) on TTP, save final adapter\n",
        "5. **Compare metrics** (Accuracy, Precision, Recall, F1) across 3 models:\n",
        "   - 1.7B **before** finetune (zero-shot)\n",
        "   - 1.7B **after** finetune (NER➜TTP sequential QLoRA adapter)\n",
        "   - 4B **before** finetune (zero-shot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate peft trl triton unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "!pip install scikit-learn seaborn matplotlib pandas numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import TextStreamer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive (if using Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    BASE_PATH = '/content/drive/MyDrive/LLM-TKIG'\n",
        "except:\n",
        "    BASE_PATH = '..'  # Local development\n",
        "\n",
        "print(f\"Base path: {BASE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "def load_json_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# NER dataset\n",
        "ner_data = load_json_data(f'{BASE_PATH}/data/entity-extraction/entity_extraction_instruction.json')\n",
        "print(f\"NER dataset size: {len(ner_data)}\")\n",
        "\n",
        "# TTP dataset\n",
        "ttp_data = load_json_data(f'{BASE_PATH}/data/TTP-classification/augmented_ttp_dataset_20250824_070057.json')\n",
        "ttp_dataset = ttp_data['dataset']\n",
        "print(f\"TTP dataset size: {len(ttp_dataset)}\")\n",
        "\n",
        "# Convert TTP to instruction format (extract only technique IDs)\n",
        "ttp_formatted = []\n",
        "for item in ttp_dataset:\n",
        "    # Extract only technique IDs and names for simpler classification\n",
        "    techniques_simplified = []\n",
        "    for technique in item['output']['techniques']:\n",
        "        techniques_simplified.append({\n",
        "            'id': technique['id'],\n",
        "            'name': technique['name']\n",
        "        })\n",
        "    \n",
        "    simplified_output = {'techniques': techniques_simplified}\n",
        "    output_str = json.dumps(simplified_output)\n",
        "    \n",
        "    ttp_formatted.append({\n",
        "        'instruction': item['instruction'],\n",
        "        'input': item['input'] if item['input'] else \"\",\n",
        "        'output': output_str\n",
        "    })\n",
        "\n",
        "print(f\"Formatted TTP dataset size: {len(ttp_formatted)}\")\n",
        "print(f\"Sample output: {ttp_formatted[0]['output']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Formatting Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alpaca prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples, tokenizer):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input_text if input_text else \"\", output) + tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Convert to datasets\n",
        "ner_dataset = Dataset.from_list(ner_data)\n",
        "ttp_dataset_obj = Dataset.from_list(ttp_formatted)\n",
        "\n",
        "print(f\"NER dataset: {ner_dataset}\")\n",
        "print(f\"TTP dataset: {ttp_dataset_obj}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Qwen3 1.7B Model (Main Training Model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen3 1.7B model for training\n",
        "model_1_7b, tokenizer_1_7b = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen3-1.7B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"Qwen3 1.7B model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Qwen3 4B Model (Baseline Comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen3 4B model for baseline comparison\n",
        "model_4b, tokenizer_4b = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen3-4B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"Qwen3 4B model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_data, task_type=\"ner\", num_samples=50):\n",
        "    \"\"\"Evaluate model performance on NER or TTP tasks\"\"\"\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    \n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    \n",
        "    for i, sample in enumerate(test_data[:num_samples]):\n",
        "        instruction = sample['instruction']\n",
        "        input_text = sample.get('input', \"\") or \"\"\n",
        "        expected_output = sample['output']\n",
        "        \n",
        "        # Generate prediction\n",
        "        inputs = tokenizer(\n",
        "            [alpaca_prompt.format(instruction, input_text, \"\")],\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256 if task_type == \"ner\" else 512,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        prediction = prediction.split(\"### Response:\")[-1].strip()\n",
        "        \n",
        "        predictions.append(prediction)\n",
        "        ground_truth.append(expected_output)\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processed {i}/{num_samples} samples\")\n",
        "    \n",
        "    return predictions, ground_truth\n",
        "\n",
        "def extract_entities_from_ner(text):\n",
        "    \"\"\"Extract entities from NER output\"\"\"\n",
        "    entities = []\n",
        "    try:\n",
        "        # Parse entities in format: (entity, type)\n",
        "        import re\n",
        "        pattern = r'\\(([^,]+),\\s*([^)]+)\\)'\n",
        "        matches = re.findall(pattern, text)\n",
        "        entities = [(match[0].strip(), match[1].strip()) for match in matches]\n",
        "    except:\n",
        "        pass\n",
        "    return entities\n",
        "\n",
        "def extract_techniques_from_ttp(text):\n",
        "    \"\"\"Extract technique IDs from TTP output\"\"\"\n",
        "    technique_ids = []\n",
        "    try:\n",
        "        import json\n",
        "        # Try to parse as JSON first\n",
        "        data = json.loads(text)\n",
        "        if 'techniques' in data:\n",
        "            technique_ids = [t.get('id', '') for t in data['techniques']]\n",
        "    except:\n",
        "        # Fallback: extract T-numbers using regex\n",
        "        import re\n",
        "        pattern = r'T\\d{4}(?:\\.\\d{3})?'\n",
        "        technique_ids = re.findall(pattern, text)\n",
        "    return technique_ids\n",
        "\n",
        "def calculate_ner_metrics(predictions, ground_truth):\n",
        "    \"\"\"Calculate NER-specific metrics\"\"\"\n",
        "    total_pred_entities = 0\n",
        "    total_true_entities = 0\n",
        "    correct_entities = 0\n",
        "    \n",
        "    for pred, gt in zip(predictions, ground_truth):\n",
        "        pred_entities = set(extract_entities_from_ner(pred))\n",
        "        true_entities = set(extract_entities_from_ner(gt))\n",
        "        \n",
        "        total_pred_entities += len(pred_entities)\n",
        "        total_true_entities += len(true_entities)\n",
        "        correct_entities += len(pred_entities.intersection(true_entities))\n",
        "    \n",
        "    # Calculate precision, recall, F1\n",
        "    precision = correct_entities / total_pred_entities if total_pred_entities > 0 else 0\n",
        "    recall = correct_entities / total_true_entities if total_true_entities > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    # Exact sequence match\n",
        "    exact_match = sum(1 for p, g in zip(predictions, ground_truth) if p.strip() == g.strip()) / len(predictions)\n",
        "    \n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'exact_match': exact_match,\n",
        "        'total_samples': len(predictions)\n",
        "    }\n",
        "\n",
        "def calculate_ttp_metrics(predictions, ground_truth):\n",
        "    \"\"\"Calculate TTP classification metrics\"\"\"\n",
        "    total_pred_techniques = 0\n",
        "    total_true_techniques = 0\n",
        "    correct_techniques = 0\n",
        "    \n",
        "    for pred, gt in zip(predictions, ground_truth):\n",
        "        pred_techniques = set(extract_techniques_from_ttp(pred))\n",
        "        true_techniques = set(extract_techniques_from_ttp(gt))\n",
        "        \n",
        "        total_pred_techniques += len(pred_techniques)\n",
        "        total_true_techniques += len(true_techniques)\n",
        "        correct_techniques += len(pred_techniques.intersection(true_techniques))\n",
        "    \n",
        "    # Calculate precision, recall, F1\n",
        "    precision = correct_techniques / total_pred_techniques if total_pred_techniques > 0 else 0\n",
        "    recall = correct_techniques / total_true_techniques if total_true_techniques > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    # Exact sequence match\n",
        "    exact_match = sum(1 for p, g in zip(predictions, ground_truth) if p.strip() == g.strip()) / len(predictions)\n",
        "    \n",
        "    # Multi-label accuracy (all techniques must match)\n",
        "    multi_label_accuracy = 0\n",
        "    for pred, gt in zip(predictions, ground_truth):\n",
        "        pred_techniques = set(extract_techniques_from_ttp(pred))\n",
        "        true_techniques = set(extract_techniques_from_ttp(gt))\n",
        "        if pred_techniques == true_techniques:\n",
        "            multi_label_accuracy += 1\n",
        "    multi_label_accuracy /= len(predictions)\n",
        "    \n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'exact_match': exact_match,\n",
        "        'multi_label_accuracy': multi_label_accuracy,\n",
        "        'total_samples': len(predictions)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zero-Shot Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== ZERO-SHOT EVALUATION ===\")\n",
        "\n",
        "# Evaluate both models on both tasks (zero-shot)\n",
        "print(\"\\n1. NER Task:\")\n",
        "pred_1_7b_ner_zero, gt_1_7b_ner_zero = evaluate_model(model_1_7b, tokenizer_1_7b, ner_data, \"ner\", 30)\n",
        "metrics_1_7b_ner_zero = calculate_ner_metrics(pred_1_7b_ner_zero, gt_1_7b_ner_zero)\n",
        "print(f\"1.7B NER Zero-shot: P={metrics_1_7b_ner_zero['precision']:.3f}, R={metrics_1_7b_ner_zero['recall']:.3f}, F1={metrics_1_7b_ner_zero['f1_score']:.3f}\")\n",
        "\n",
        "pred_4b_ner_zero, gt_4b_ner_zero = evaluate_model(model_4b, tokenizer_4b, ner_data, \"ner\", 30)\n",
        "metrics_4b_ner_zero = calculate_ner_metrics(pred_4b_ner_zero, gt_4b_ner_zero)\n",
        "print(f\"4B NER Zero-shot: P={metrics_4b_ner_zero['precision']:.3f}, R={metrics_4b_ner_zero['recall']:.3f}, F1={metrics_4b_ner_zero['f1_score']:.3f}\")\n",
        "\n",
        "print(\"\\n2. TTP Task:\")\n",
        "pred_1_7b_ttp_zero, gt_1_7b_ttp_zero = evaluate_model(model_1_7b, tokenizer_1_7b, ttp_formatted, \"ttp\", 30)\n",
        "metrics_1_7b_ttp_zero = calculate_ttp_metrics(pred_1_7b_ttp_zero, gt_1_7b_ttp_zero)\n",
        "print(f\"1.7B TTP Zero-shot: P={metrics_1_7b_ttp_zero['precision']:.3f}, R={metrics_1_7b_ttp_zero['recall']:.3f}, F1={metrics_1_7b_ttp_zero['f1_score']:.3f}, MLA={metrics_1_7b_ttp_zero['multi_label_accuracy']:.3f}\")\n",
        "\n",
        "pred_4b_ttp_zero, gt_4b_ttp_zero = evaluate_model(model_4b, tokenizer_4b, ttp_formatted, \"ttp\", 30)\n",
        "metrics_4b_ttp_zero = calculate_ttp_metrics(pred_4b_ttp_zero, gt_4b_ttp_zero)\n",
        "print(f\"4B TTP Zero-shot: P={metrics_4b_ttp_zero['precision']:.3f}, R={metrics_4b_ttp_zero['recall']:.3f}, F1={metrics_4b_ttp_zero['f1_score']:.3f}, MLA={metrics_4b_ttp_zero['multi_label_accuracy']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup QLoRA for Sequential Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add LoRA adapters to 1.7B model\n",
        "model_1_7b = FastLanguageModel.get_peft_model(\n",
        "    model_1_7b,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters added to Qwen3 1.7B model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: NER Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format NER dataset and train\n",
        "ner_dataset_formatted = ner_dataset.map(\n",
        "    lambda examples: formatting_prompts_func(examples, tokenizer_1_7b),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "trainer_ner = SFTTrainer(\n",
        "    model=model_1_7b,\n",
        "    tokenizer=tokenizer_1_7b,\n",
        "    train_dataset=ner_dataset_formatted,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_ner\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Starting NER training...\")\n",
        "trainer_stats_ner = trainer_ner.train()\n",
        "print(f\"NER training completed in {trainer_stats_ner.metrics['train_runtime']:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sequential TTP Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format TTP dataset and continue training on same adapter\n",
        "ttp_dataset_formatted = ttp_dataset_obj.map(\n",
        "    lambda examples: formatting_prompts_func(examples, tokenizer_1_7b),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "trainer_ttp = SFTTrainer(\n",
        "    model=model_1_7b,  # Same model with NER adapter\n",
        "    tokenizer=tokenizer_1_7b,\n",
        "    train_dataset=ttp_dataset_formatted,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=150,\n",
        "        learning_rate=1e-4,  # Lower LR for sequential training\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_ttp\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Starting TTP training (sequential after NER)...\")\n",
        "trainer_stats_ttp = trainer_ttp.train()\n",
        "print(f\"TTP training completed in {trainer_stats_ttp.metrics['train_runtime']:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Sequential Adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final sequential adapter (NER → TTP)\n",
        "final_adapter_path = f\"{BASE_PATH}/models/qwen3_1_7b_ner_ttp_sequential_adapter\"\n",
        "model_1_7b.save_pretrained(final_adapter_path)\n",
        "tokenizer_1_7b.save_pretrained(final_adapter_path)\n",
        "\n",
        "print(f\"Sequential adapter saved to: {final_adapter_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Training Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== POST-TRAINING EVALUATION ===\")\n",
        "\n",
        "# Evaluate fine-tuned 1.7B model on both tasks\n",
        "print(\"\\n1. NER Task (After Sequential Training):\")\n",
        "pred_1_7b_ner_ft, gt_1_7b_ner_ft = evaluate_model(model_1_7b, tokenizer_1_7b, ner_data, \"ner\", 30)\n",
        "metrics_1_7b_ner_ft = calculate_ner_metrics(pred_1_7b_ner_ft, gt_1_7b_ner_ft)\n",
        "print(f\"1.7B NER After Training: P={metrics_1_7b_ner_ft['precision']:.3f}, R={metrics_1_7b_ner_ft['recall']:.3f}, F1={metrics_1_7b_ner_ft['f1_score']:.3f}\")\n",
        "\n",
        "print(\"\\n2. TTP Task (After Sequential Training):\")\n",
        "pred_1_7b_ttp_ft, gt_1_7b_ttp_ft = evaluate_model(model_1_7b, tokenizer_1_7b, ttp_formatted, \"ttp\", 30)\n",
        "metrics_1_7b_ttp_ft = calculate_ttp_metrics(pred_1_7b_ttp_ft, gt_1_7b_ttp_ft)\n",
        "print(f\"1.7B TTP After Training: P={metrics_1_7b_ttp_ft['precision']:.3f}, R={metrics_1_7b_ttp_ft['recall']:.3f}, F1={metrics_1_7b_ttp_ft['f1_score']:.3f}, MLA={metrics_1_7b_ttp_ft['multi_label_accuracy']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Comparison and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile and display results\n",
        "results_summary = {\n",
        "    'Model': [\n",
        "        'Qwen3 1.7B (Zero-shot)',\n",
        "        'Qwen3 1.7B (After Sequential FT)',\n",
        "        'Qwen3 4B (Zero-shot)'\n",
        "    ],\n",
        "    'NER_Precision': [\n",
        "        metrics_1_7b_ner_zero['precision'],\n",
        "        metrics_1_7b_ner_ft['precision'],\n",
        "        metrics_4b_ner_zero['precision']\n",
        "    ],\n",
        "    'NER_Recall': [\n",
        "        metrics_1_7b_ner_zero['recall'],\n",
        "        metrics_1_7b_ner_ft['recall'],\n",
        "        metrics_4b_ner_zero['recall']\n",
        "    ],\n",
        "    'NER_F1': [\n",
        "        metrics_1_7b_ner_zero['f1_score'],\n",
        "        metrics_1_7b_ner_ft['f1_score'],\n",
        "        metrics_4b_ner_zero['f1_score']\n",
        "    ],\n",
        "    'TTP_Precision': [\n",
        "        metrics_1_7b_ttp_zero['precision'],\n",
        "        metrics_1_7b_ttp_ft['precision'],\n",
        "        metrics_4b_ttp_zero['precision']\n",
        "    ],\n",
        "    'TTP_Recall': [\n",
        "        metrics_1_7b_ttp_zero['recall'],\n",
        "        metrics_1_7b_ttp_ft['recall'],\n",
        "        metrics_4b_ttp_zero['recall']\n",
        "    ],\n",
        "    'TTP_F1': [\n",
        "        metrics_1_7b_ttp_zero['f1_score'],\n",
        "        metrics_1_7b_ttp_ft['f1_score'],\n",
        "        metrics_4b_ttp_zero['f1_score']\n",
        "    ],\n",
        "    'TTP_MultiLabel_Acc': [\n",
        "        metrics_1_7b_ttp_zero['multi_label_accuracy'],\n",
        "        metrics_1_7b_ttp_ft['multi_label_accuracy'],\n",
        "        metrics_4b_ttp_zero['multi_label_accuracy']\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "print(\"\\n=== FINAL RESULTS SUMMARY ===\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Performance visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Qwen3 Sequential QLoRA Fine-tuning Results', fontsize=16)\n",
        "\n",
        "# Plot comparisons\n",
        "colors = ['red', 'blue', 'green']\n",
        "model_labels = [label.replace('Qwen3 ', '').replace(' (Zero-shot)', ' (Z)').replace(' (After Sequential FT)', ' (FT)') for label in results_df['Model']]\n",
        "\n",
        "# NER metrics\n",
        "axes[0,0].bar(model_labels, results_df['NER_Precision'], color=colors)\n",
        "axes[0,0].set_title('NER Precision')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[0,1].bar(model_labels, results_df['NER_Recall'], color=colors)\n",
        "axes[0,1].set_title('NER Recall')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[0,2].bar(model_labels, results_df['NER_F1'], color=colors)\n",
        "axes[0,2].set_title('NER F1-Score')\n",
        "axes[0,2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# TTP metrics\n",
        "axes[1,0].bar(model_labels, results_df['TTP_Precision'], color=colors)\n",
        "axes[1,0].set_title('TTP Precision')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[1,1].bar(model_labels, results_df['TTP_Recall'], color=colors)\n",
        "axes[1,1].set_title('TTP Recall')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[1,2].bar(model_labels, results_df['TTP_F1'], color=colors)\n",
        "axes[1,2].set_title('TTP F1-Score')\n",
        "axes[1,2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_df.to_csv(f\"{BASE_PATH}/results/qwen3_sequential_results_{timestamp}.csv\", index=False)\n",
        "print(f\"\\nResults saved to: {BASE_PATH}/results/qwen3_sequential_results_{timestamp}.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Summary and Key Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== TRAINING SUMMARY ===\")\n",
        "print(f\"NER Training Time: {trainer_stats_ner.metrics['train_runtime']:.1f} seconds\")\n",
        "print(f\"TTP Training Time: {trainer_stats_ttp.metrics['train_runtime']:.1f} seconds\")\n",
        "print(f\"Total Training Time: {trainer_stats_ner.metrics['train_runtime'] + trainer_stats_ttp.metrics['train_runtime']:.1f} seconds\")\n",
        "\n",
        "print(\"\\n=== KEY INSIGHTS ===\")\n",
        "print(f\"1. NER F1 Improvement (1.7B): {metrics_1_7b_ner_ft['f1_score'] - metrics_1_7b_ner_zero['f1_score']:.3f}\")\n",
        "print(f\"2. TTP F1 Improvement (1.7B): {metrics_1_7b_ttp_ft['f1_score'] - metrics_1_7b_ttp_zero['f1_score']:.3f}\")\n",
        "print(f\"3. 1.7B vs 4B NER F1 (Zero-shot): {metrics_4b_ner_zero['f1_score'] - metrics_1_7b_ner_zero['f1_score']:.3f}\")\n",
        "print(f\"4. 1.7B vs 4B TTP F1 (Zero-shot): {metrics_4b_ttp_zero['f1_score'] - metrics_1_7b_ttp_zero['f1_score']:.3f}\")\n",
        "print(f\"5. 1.7B Fine-tuned vs 4B Zero-shot (NER F1): {metrics_1_7b_ner_ft['f1_score'] - metrics_4b_ner_zero['f1_score']:.3f}\")\n",
        "print(f\"6. 1.7B Fine-tuned vs 4B Zero-shot (TTP F1): {metrics_1_7b_ttp_ft['f1_score'] - metrics_4b_ttp_zero['f1_score']:.3f}\")\n",
        "print(f\"7. TTP Multi-label Accuracy Improvement: {metrics_1_7b_ttp_ft['multi_label_accuracy'] - metrics_1_7b_ttp_zero['multi_label_accuracy']:.3f}\")\n",
        "\n",
        "print(\"\\n=== CONCLUSION ===\")\n",
        "print(\"Sequential QLoRA fine-tuning allows smaller models to compete with larger ones.\")\n",
        "print(\"The 1.7B model after fine-tuning shows significant improvements on both tasks.\")\n",
        "print(\"This approach demonstrates efficient use of computational resources for specialized tasks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
