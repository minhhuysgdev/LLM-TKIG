{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entity and Relationship Extraction for Threat Intelligence\n",
    "\n",
    "## Overview\n",
    "This notebook implements entity and relationship extraction from threat intelligence text using LLM-based approach.\n",
    "\n",
    "### Task Description\n",
    "- **Input**: Threat intelligence text content\n",
    "- **Output**: Named entities and relationships in structured format\n",
    "- **Entity Types**: malware, threat type, attacker, vulnerability, tool, etc.\n",
    "- **Relationship Types**: use, target, exploit, etc.\n",
    "\n",
    "### Example\n",
    "**Input**: A hitherto unknown attack group has been observed targeting a materials research organization in Asia. The group, which Symantec calls Clasiopa, is characterized by a distinct toolset, which includes one piece of custom malware (Backdoor.Atharvan).\n",
    "\n",
    "**Output**:\n",
    "- Named Entities: (Clasiopa, attacker), (custom malware, malware), (Backdoor.Atharvan, malware)\n",
    "- Relationships: (Clasiopa, use, custom malware), (custom malware, name, Backdoor.Atharvan)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:35:15.095393Z",
     "start_time": "2025-08-03T12:35:04.472875Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "# Load environment and model setup\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîß Setting up Entity & Relationship Extraction Pipeline\")\n",
    "print(\"=\" * 60)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up Entity & Relationship Extraction Pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:36:13.822197Z",
     "start_time": "2025-08-03T12:36:13.787544Z"
    }
   },
   "source": [
    "def load_data(input_file: str) -> list:\n",
    "    \"\"\"\n",
    "    Load threat intelligence data from JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded {len(data)} records from {input_file}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {input_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load threat intelligence data\n",
    "data_path = '../data/processed/merged_threat_intelligence.json'\n",
    "data = load_data(data_path)\n",
    "\n",
    "if data:\n",
    "    print(f\"üìä Sample data structure:\")\n",
    "    print(f\"   Keys: {list(data[0].keys())}\")\n",
    "    print(f\"   Title: {data[0]['title'][:100]}...\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 427 records from ../data/processed/merged_threat_intelligence.json\n",
      "üìä Sample data structure:\n",
      "   Keys: ['title', 'content', 'link']\n",
      "   Title: FortiGuard Labs Threat Research...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:36:17.728273Z",
     "start_time": "2025-08-03T12:36:16.669049Z"
    }
   },
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"üñ•Ô∏è  Using device: {device.upper()}\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Memory cleanup\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if hasattr(torch.mps, 'empty_cache'):\n",
    "        torch.mps.empty_cache()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: CUDA\n",
      "üîß PyTorch version: 2.7.1+cu128\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:36:27.113602Z",
     "start_time": "2025-08-03T12:36:20.299746Z"
    }
   },
   "source": [
    "# Get configuration from environment\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')\n",
    "FALLBACK_MODEL = os.getenv('FALLBACK_MODEL', 'gpt2')\n",
    "\n",
    "def setup_model_for_extraction(model_name: str = None, hf_token: str = None):\n",
    "    \"\"\"\n",
    "    T·∫£i model t·ª´ Hugging Face v·ªõi token t·ª´ environment variables.\n",
    "    \"\"\"\n",
    "    model_name = model_name or DEFAULT_MODEL\n",
    "    hf_token = hf_token or HF_TOKEN\n",
    "\n",
    "    print(f\"ü§ñ ƒêang t·∫£i m√¥ h√¨nh: {model_name}\")\n",
    "    print(f\"üì± Thi·∫øt b·ªã: {device.upper()}\")\n",
    "    print(f\"üîë Token: {'‚úÖ Found' if hf_token else '‚ùå Missing'}\")\n",
    "\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            token=hf_token,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "\n",
    "        # Thi·∫øt l·∫≠p ki·ªÉu d·ªØ li·ªáu v√† b·∫£n ƒë·ªì thi·∫øt b·ªã\n",
    "        torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "        device_map = \"auto\" if device == \"cuda\" else None\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=hf_token,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=device_map,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        if device_map is None and device in [\"mps\", \"cuda\"]:\n",
    "            model.to(device)\n",
    "\n",
    "        if device_map is None:\n",
    "            # N·∫øu kh√¥ng s·ª≠ d·ª•ng device_map=\"auto\", c√≥ th·ªÉ ch·ªâ ƒë·ªãnh device\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=0 if device != \"cpu\" else -1,\n",
    "                torch_dtype=torch_dtype,\n",
    "                model_kwargs={\"use_cache\": False}\n",
    "            )\n",
    "        else:\n",
    "            # N·∫øu s·ª≠ d·ª•ng device_map=\"auto\", kh√¥ng ch·ªâ ƒë·ªãnh device\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch_dtype,\n",
    "                model_kwargs={\"use_cache\": False}\n",
    "            )\n",
    "\n",
    "        print(f\"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {model_name} tr√™n {device.upper()}\")\n",
    "        return pipe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t·∫£i {model_name}: {e}\")\n",
    "        return setup_fallback_model(hf_token)\n",
    "\n",
    "def setup_fallback_model(hf_token: str = None):\n",
    "    \"\"\"\n",
    "    T·∫£i fallback model n·∫øu model ch√≠nh l·ªói.\n",
    "    \"\"\"\n",
    "    fallback_name = FALLBACK_MODEL\n",
    "    hf_token = hf_token or HF_TOKEN\n",
    "    print(f\"üîÑ ƒêang t·∫£i m√¥ h√¨nh d·ª± ph√≤ng: {fallback_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(fallback_name, token=hf_token)\n",
    "        tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            fallback_name,\n",
    "            token=hf_token,\n",
    "            torch_dtype=torch.float32,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        if device in [\"cuda\", \"mps\"]:\n",
    "            model.to(device)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if device != \"cpu\" else -1,\n",
    "            model_kwargs={\"use_cache\": False}\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ {FALLBACK_MODEL} ƒë√£ s·∫µn s√†ng tr√™n {device.upper()}\")\n",
    "        return pipe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t·∫£i {FALLBACK_MODEL} fallback: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load model\n",
    "extraction_model = setup_model_for_extraction()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ ƒêang t·∫£i m√¥ h√¨nh: Qwen/Qwen2.5-1.5B-Instruct\n",
      "üì± Thi·∫øt b·ªã: CUDA\n",
      "üîë Token: ‚úÖ Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng Qwen/Qwen2.5-1.5B-Instruct tr√™n CUDA\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:36:29.014539Z",
     "start_time": "2025-08-03T12:36:28.997333Z"
    }
   },
   "source": [
    "def create_entity_extraction_prompt(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create prompt for entity and relationship extraction focusing on core cybersecurity entity types.\n",
    "    \"\"\"\n",
    "    # Truncate text to avoid token limits\n",
    "    text_truncated = (text[:1500] if text else \"\").replace('\\n', ' ').strip()\n",
    "    \n",
    "    prompt = f\"\"\"Instruction: Please identify the following types of entities and then extract the relationships between these extracted entities:\n",
    "\n",
    "Entity Types (focus on these only):\n",
    "- Malware: Malicious software (e.g., 'Stuxnet', 'Emotet', 'Backdoor.Atharvan')\n",
    "- Threat Type: Category of threats (e.g., 'Ransomware', 'APT', 'Botnet')\n",
    "- Attacker: Threat actors/groups (e.g., 'APT28', 'Lazarus Group', 'Shuckworm')\n",
    "- Technique: Attack techniques/TTPs (e.g., 'T1057: Process Discovery', 'Privilege Escalation', 'Phishing')\n",
    "- Tool: Security tools or attack tools (e.g., 'PowerShell', 'Cobalt Strike', 'EHole')\n",
    "- Vulnerability: Security weaknesses (e.g., 'CVE-2020-1472', 'CVE-2021-44228')\n",
    "- IP: IP addresses (e.g., '45.153.243.93', '192.168.1.100')\n",
    "- Domain: Domain names (e.g., 'malicious-domain[.]com', 'evil[.]example[.]com')\n",
    "- URL: URLs (e.g., 'hxxp://178.73.192[.]15/cal.exe')\n",
    "- File: File names (e.g., 'rtk.lnk', 'payload.exe', 'shtasks.exe')\n",
    "- Hash: File hashes (e.g., '2aee8bb2a953124803bc42e5c42935c9', MD5/SHA1/SHA256)\n",
    "\n",
    "Relationship Types:\n",
    "- use, hash, aka, execute, used by, download, resolved to, IP, drop, associated with, deploy, communicate with, connect to, install, exploit, contain, run, launch, target, linked to\n",
    "\n",
    "If there are no entities and relationships pertaining to the specified types, please state 'No related entities and relations'. Make sure to follow the output format shown in the following examples.\n",
    "\n",
    "Example 1:\n",
    "Input: A hitherto unknown attack group has been observed targeting a materials research organization in Asia. The group, which Symantec calls Clasiopa, is characterized by a distinct toolset, which includes one piece of custom malware (Backdoor.Atharvan).\n",
    "Output: Named Entities: (Clasiopa, Attacker), (Backdoor.Atharvan, Malware)\\\\nRelationships: (Clasiopa, uses, Backdoor.Atharvan)\n",
    "\n",
    "Example 2:\n",
    "Input: The Emotet malware has been observed using new phishing techniques to target banking institutions. The malware exploits CVE-2021-1234 vulnerability in Microsoft Office.\n",
    "Output: Named Entities: (Emotet, Malware), (phishing, Technique), (CVE-2021-1234, Vulnerability), (Microsoft Office, Tool)\\\\nRelationships: (Emotet, uses, phishing), (Emotet, exploits, CVE-2021-1234)\n",
    "\n",
    "Example 3:\n",
    "Input: The threat actor downloaded malicious payload from hxxp://malicious-domain[.]com/payload.exe and used hash 2aee8bb2a953124803bc42e5c42935c9 to verify file integrity. The attack targeted IP address 192.168.1.100.\n",
    "Output: Named Entities: (threat actor, Attacker), (malicious payload, File), (hxxp://malicious-domain[.]com/payload.exe, URL), (2aee8bb2a953124803bc42e5c42935c9, Hash), (192.168.1.100, IP)\\\\nRelationships: (threat actor, uses, hxxp://malicious-domain[.]com/payload.exe), (threat actor, targets, 192.168.1.100)\n",
    "\n",
    "Example 4:\n",
    "Input: H2Miner botnet uses Kinsing malware and Cobalt Strike to deploy XMRig miners. The campaign communicates with C2 server at evil[.]domain[.]com and is attributed to APT group.\n",
    "Output: Named Entities: (H2Miner, Threat Type), (Kinsing, Malware), (Cobalt Strike, Tool), (XMRig, Tool), (evil[.]domain[.]com, Domain), (APT group, Attacker)\\\\nRelationships: (H2Miner, uses, Kinsing), (H2Miner, uses, Cobalt Strike), (H2Miner, uses, XMRig), (Kinsing, communicatesWith, evil[.]domain[.]com), (H2Miner, attributedTo, APT group)\n",
    "\n",
    "Example 5:\n",
    "Input: The weather forecast shows sunny skies and moderate temperatures for the weekend.\n",
    "Output: No related entities and relations\n",
    "\n",
    "Now extract entities and relationships from the following text:\n",
    "Input: {text_truncated}\n",
    "Output:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test the prompt creation\n",
    "if data:\n",
    "    sample_prompt = create_entity_extraction_prompt(data[0]['content'])\n",
    "    print(\"üìù Sample prompt (first 500 chars):\")\n",
    "    print(sample_prompt[:500] + \"...\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample prompt (first 500 chars):\n",
      "Instruction: Please identify the following types of entities and then extract the relationships between these extracted entities:\n",
      "\n",
      "Entity Types (focus on these only):\n",
      "- Malware: Malicious software (e.g., 'Stuxnet', 'Emotet', 'Backdoor.Atharvan')\n",
      "- Threat Type: Category of threats (e.g., 'Ransomware', 'APT', 'Botnet')\n",
      "- Attacker: Threat actors/groups (e.g., 'APT28', 'Lazarus Group', 'Shuckworm')\n",
      "- Technique: Attack techniques/TTPs (e.g., 'T1057: Process Discovery', 'Privilege Escalation', 'Phishi...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:37:32.251387Z",
     "start_time": "2025-08-03T12:36:44.955012Z"
    }
   },
   "source": [
    "def extract_entities_and_relationships(pipe, text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract entities and relationships from text using the LLM.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = create_entity_extraction_prompt(text)\n",
    "        \n",
    "        # Generate response\n",
    "        response = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        # Extract generated text\n",
    "        generated_text = response[0]['generated_text']\n",
    "        answer = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"üîç Raw model output: {answer[:200]}...\")\n",
    "        \n",
    "        # Parse the response\n",
    "        entities, relationships = parse_extraction_output(answer)\n",
    "        \n",
    "        return {\n",
    "            \"raw_output\": answer,\n",
    "            \"entities\": entities,\n",
    "            \"relationships\": relationships,\n",
    "            \"has_entities\": len(entities) > 0\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in extraction: {e}\")\n",
    "        return {\n",
    "            \"raw_output\": \"\",\n",
    "            \"entities\": [],\n",
    "            \"relationships\": [],\n",
    "            \"has_entities\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def parse_extraction_output(output: str) -> Tuple[List[Tuple], List[Tuple]]:\n",
    "    \"\"\"\n",
    "    Parse the model output to extract entities and relationships.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    relationships = []\n",
    "    \n",
    "    # Check for \"No related entities\" case\n",
    "    if \"no related entities\" in output.lower():\n",
    "        return entities, relationships\n",
    "    \n",
    "    try:\n",
    "        # Split output into lines\n",
    "        lines = [line.strip() for line in output.split('\\n') if line.strip()]\n",
    "        \n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            line_lower = line.lower()\n",
    "            \n",
    "            if \"named entities:\" in line_lower:\n",
    "                current_section = \"entities\"\n",
    "                # Extract entities from the same line\n",
    "                entity_part = line.split(\":\", 1)[1] if \":\" in line else \"\"\n",
    "                entities.extend(extract_tuples_from_text(entity_part))\n",
    "                \n",
    "            elif \"relationships:\" in line_lower:\n",
    "                current_section = \"relationships\"\n",
    "                # Extract relationships from the same line\n",
    "                rel_part = line.split(\":\", 1)[1] if \":\" in line else \"\"\n",
    "                relationships.extend(extract_tuples_from_text(rel_part))\n",
    "                \n",
    "            elif current_section == \"entities\":\n",
    "                entities.extend(extract_tuples_from_text(line))\n",
    "                \n",
    "            elif current_section == \"relationships\":\n",
    "                relationships.extend(extract_tuples_from_text(line))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error parsing output: {e}\")\n",
    "    \n",
    "    return entities, relationships\n",
    "\n",
    "def extract_tuples_from_text(text: str) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Extract tuples from text using regex pattern matching.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    \n",
    "    # Pattern to match (item1, item2) or (item1, item2, item3)\n",
    "    pattern = r'\\(([^)]+)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    for match in matches:\n",
    "        # Split by comma and clean up\n",
    "        parts = [part.strip() for part in match.split(',')]\n",
    "        if len(parts) >= 2:\n",
    "            tuples.append(tuple(parts))\n",
    "    \n",
    "    return tuples\n",
    "\n",
    "# Test the extraction function\n",
    "if extraction_model and data:\n",
    "    print(\"\\nüß™ Testing entity extraction on sample data...\")\n",
    "    test_result = extract_entities_and_relationships(extraction_model, data[0]['content'])\n",
    "    \n",
    "    print(f\"\\nüìä Extraction Results:\")\n",
    "    print(f\"   Entities found: {len(test_result['entities'])}\")\n",
    "    print(f\"   Relationships found: {len(test_result['relationships'])}\")\n",
    "    \n",
    "    if test_result['entities']:\n",
    "        print(\"\\nüè∑Ô∏è  Sample Entities:\")\n",
    "        for entity in test_result['entities'][:5]:\n",
    "            print(f\"     {entity}\")\n",
    "    \n",
    "    if test_result['relationships']:\n",
    "        print(\"\\nüîó Sample Relationships:\")\n",
    "        for rel in test_result['relationships'][:5]:\n",
    "            print(f\"     {rel}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing entity extraction on sample data...\n",
      "üîç Raw model output: Named Entities: (NailaoLocker, Malware), (SM2, Technique), (Lcrypt0rx, Malware), (Dark 101, Malware), (FortiCNAPP Composite Alerts, Tool), (Lcrypt0rx, Malware), (FortiCNAPP Labs, Tool), (FortiSandbox ...\n",
      "\n",
      "üìä Extraction Results:\n",
      "   Entities found: 12\n",
      "   Relationships found: 9\n",
      "\n",
      "üè∑Ô∏è  Sample Entities:\n",
      "     ('NailaoLocker', 'Malware')\n",
      "     ('SM2', 'Technique')\n",
      "     ('Lcrypt0rx', 'Malware')\n",
      "     ('Dark 101', 'Malware')\n",
      "     ('FortiCNAPP Composite Alerts', 'Tool')\n",
      "\n",
      "üîó Sample Relationships:\n",
      "     ('NailaoLocker', 'uses', 'SM2')\n",
      "     ('NailaoLocker', 'contains', 'Lcrypt0rx')\n",
      "     ('Lcrypt0rx', 'uses', 'Dark 101')\n",
      "     ('FortiCNAPP Composite Alerts', 'linksWeakSignalsIntoClearTimelines')\n",
      "     ('Lcrypt0rx', 'uses', 'FortiCNAPP Labs')\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:41:14.881331Z",
     "start_time": "2025-08-03T12:37:38.249550Z"
    }
   },
   "source": [
    "def process_articles_for_extraction(data: List[Dict], pipe, start: int = 0, offset:int=5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process multiple articles for entity and relationship extraction.\n",
    "    \"\"\"\n",
    "    end = min(start + offset, len(data))\n",
    "    articles_to_process = data[start:end]\n",
    "    results = []\n",
    "\n",
    "    print(f\"üîç Processing {len(articles_to_process)} articles for entity extraction...\")\n",
    "\n",
    "    for i, article in enumerate(articles_to_process):\n",
    "        print(f\"\\nProcessing {i+1}/{len(articles_to_process)}: {article.get('title', 'Unknown')[:60]}...\")\n",
    "\n",
    "        # Extract entities and relationships\n",
    "        extraction_result = extract_entities_and_relationships(pipe, article.get('content', ''))\n",
    "\n",
    "        # Combine with original article data\n",
    "        result = {\n",
    "            \"title\": article.get('title', ''),\n",
    "            \"link\": article.get('link', ''),\n",
    "            \"content\": article.get('content', ''),\n",
    "            \"extraction\": extraction_result,\n",
    "            \"entity_count\": len(extraction_result['entities']),\n",
    "            \"relationship_count\": len(extraction_result['relationships'])\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  ‚úÖ Processed {i+1}/{len(articles_to_process)} articles\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Process a small batch first for testing\n",
    "print(\"\\nüöÄ Processing first 5 articles for entity extraction...\")\n",
    "extraction_results = process_articles_for_extraction(data, extraction_model, start=0, offset = 5)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing first 5 articles for entity extraction...\n",
      "üîç Processing 5 articles for entity extraction...\n",
      "\n",
      "Processing 1/5: FortiGuard Labs Threat Research...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw model output: Named Entities: (NailaoLocker, Malware), (SM2, Technique), (Lcrypt0rx, Malware), (Dark 101, Malware), (FortiCNAPP Composite Alerts, Tool), (Lcrypt0rx, Malware), (FortiCNAPP Labs, Tool), (FortiSandbox ...\n",
      "\n",
      "Processing 2/5: NailaoLocker Ransomware‚Äôs ‚ÄúCheese‚Äù...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw model output: Named Entities: (FortiGuard Labs Threat Research, Entity), (NailaoLocker, Malware), (AES-256-CBC, Technique), (SM2 cryptographic key, Vulnerability), (Windows, Platform), (user files, File), (high sev...\n",
      "\n",
      "Processing 3/5: Improving Cloud Intrusion Detection and Triage with FortiCNA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw model output: Named Entities: (FortiGuard Labs Threat Research, Entity), (Cloud, Entity), (Multi-stage technique, Technique), (Authentication abuse, Technique), (Privilege escalation, Technique), (Command execution...\n",
      "\n",
      "Processing 4/5: Old Miner, New Tricks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw model output: Named Entities: (FortiCNAPP team, Attacker), (H2miner, Threat Type), (Lcrypt0rx, Malware), (Linux, OS), (Windows, OS), (Containers, OS), (KinSing, Tool), (Xmrig miners, Tool), (Lcrypt0rx, Malware), (L...\n",
      "\n",
      "Processing 5/5: How FortiSandbox 5.0 Detects Dark 101 Ransomware Despite Eva...\n",
      "üîç Raw model output: Named Entities: (FortiGuard Labs, Organization), (Dark 101, Malware Family), (ransomware, Threat Type), (ransomnote, Threat Type), (Bitcoin, Currency), (Task Manager, Tool), (backupcatalog, Object), (...\n",
      "  ‚úÖ Processed 5/5 articles\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T13:02:53.116487Z",
     "start_time": "2025-08-03T13:02:53.111481Z"
    }
   },
   "source": [
    "def save_extraction_results(results: List[Dict], output_file: str = \"entity-extraction.json\"):\n",
    "    \"\"\"\n",
    "    Save extraction results to files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert to an absolute path to avoid relative path issues\n",
    "        absolute_path = output_path.resolve()\n",
    "        print(f\"üíæ Saving to: {absolute_path}\")\n",
    "        \n",
    "        with open(absolute_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ SAVED EXTRACTION RESULTS to {output_file}\")\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "        print(f\"   Attempted path: {output_path}\")\n",
    "        print(f\"   Current working directory: {os.getcwd()}\")\n",
    "        print(f\"   Absolute path would be: {Path(output_path).resolve()}\")\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:55:25.833247Z",
     "start_time": "2025-08-03T12:54:10.848449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test save results\n",
    "import datetime\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "offset = 2\n",
    "start = 0\n",
    "end = min(len(data), start+offset)\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 2 articles for entity extraction...\n",
      "\n",
      "Processing 1/2: FortiGuard Labs Threat Research...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw model output: Named Entities: (NailaoLocker, Malware), (SM2, Technique), (Lcrypt0rx, Malware), (Dark 101, Malware), (FortiCNAPP Composite Alerts, Tool), (Lcrypt0rx, Malware), (FortiCNAPP Labs, Tool), (FortiSandbox ...\n",
      "\n",
      "Processing 2/2: NailaoLocker Ransomware‚Äôs ‚ÄúCheese‚Äù...\n",
      "üîç Raw model output: Named Entities: (FortiGuard Labs Threat Research, Entity), (NailaoLocker, Malware), (AES-256-CBC, Technique), (SM2 cryptographic key, Vulnerability), (Windows, Platform), (user files, File), (high sev...\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T13:02:56.024518Z",
     "start_time": "2025-08-03T13:02:56.008882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = f\"../data/entity-extraction/entity-extraction_{today}_{start}_{end}.json\"\n",
    "save_extraction_results(results, output_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to: C:\\Users\\KietVu\\Testplace\\LLM_TKIG\\data\\entity-extraction\\entity-extraction_2025-08-03_19-54-10_0_2.json\n",
      "\n",
      "üíæ SAVED EXTRACTION RESULTS to ../data/entity-extraction/entity-extraction_2025-08-03_19-54-10_0_2.json\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "offset = 50\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = 0\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 50\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 100\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 150\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 200\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 250\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 300\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 350\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start = 400\n",
    "end = min(len(data), start+offset)\n",
    "output_path = f\"../data/entity-extraction/topic_classification_{os.getenv('DEFAULT_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')}_test_{today}_{start}_{end}.json\"\n",
    "\n",
    "results = process_articles_for_extraction(data, extraction_model,start=start,offset=offset)\n",
    "save_extraction_results(results, output_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
